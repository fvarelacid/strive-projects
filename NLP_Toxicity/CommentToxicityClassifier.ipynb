{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9gpXZ2UOOD0n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import shutil\n",
        "import sys\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZdJ01WisPBC1"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('data/train.csv')\n",
        "df_test = pd.read_csv('data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HjGt1YBYPcai",
        "outputId": "b8730ac7-630d-4950-c7d6-41989e1c33c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d aww he matches this background colour i am s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hey man i am really not trying to edit war it ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>more i can not make any real suggestions on im...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  ...  identity_hate\n",
              "0  explanation why the edits made under my userna...  ...              0\n",
              "1  d aww he matches this background colour i am s...  ...              0\n",
              "2  hey man i am really not trying to edit war it ...  ...              0\n",
              "3  more i can not make any real suggestions on im...  ...              0\n",
              "4  you sir are my hero any chance you remember wh...  ...              0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B_fvXYj2PfcK"
      },
      "outputs": [],
      "source": [
        "labels_list = ['toxic',\t'severe_toxic',\t'obscene',\t'threat',\t'insult', 'identity_hate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ARBBaOZ4Pqwn"
      },
      "outputs": [],
      "source": [
        "MAX_TOKEN_LEN = 512\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qYfG1qWAP_HI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscovarelacid/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sZlesv-IcIi1"
      },
      "outputs": [],
      "source": [
        "class CommentsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, tokenizer, max_token_len):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "    self.comment = self.df['comment_text']\n",
        "    self.labels = self.df[labels_list].values\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.comment)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    comment = str(self.comment)\n",
        "    \n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "        comment,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_token_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'].flatten(),\n",
        "        'attention_mask': inputs['attention_mask'].flatten(),\n",
        "        'targets': torch.FloatTensor(self.labels[index])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uKU3IkY2XTJN"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df_train, test_size=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ekrBVvO6XTGG"
      },
      "outputs": [],
      "source": [
        "train_toxic = train_df[train_df[labels_list].sum(axis=1) > 0]\n",
        "train_clean = train_df[train_df[labels_list].sum(axis=1) == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S5CvWWCsXTAA"
      },
      "outputs": [],
      "source": [
        "train_df = pd.concat([\n",
        "  train_toxic,\n",
        "  train_clean.sample(20_000)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-af4JAUbXS5-"
      },
      "outputs": [],
      "source": [
        "train_dataset = CommentsDataset(train_df, tokenizer, MAX_TOKEN_LEN)\n",
        "val_dataset = CommentsDataset(val_df, tokenizer, MAX_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vr8rIoGPXSyO"
      },
      "outputs": [],
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    shuffle=False,\n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Sp8YLg-Gdryr"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iisGfGKzdrrE"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  valid_loss_min = checkpoint['valid_loss_min']\n",
        "  return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint_path, best_model_path):\n",
        "  f_path = checkpoint_path\n",
        "  torch.save(state, f_path)\n",
        "  if is_best:\n",
        "    best_path = best_model_path\n",
        "    shutil.copyfile(f_path, best_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wCgu_2t5drmv"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.linear = nn.Linear(768, 6)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "    output_dropout = self.dropout(output.pooler_output)\n",
        "    output = self.linear(output_dropout)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYklmnCgXSvC"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mJZXqlPDXSrE"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "  return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "93mqM1NWXSnr"
      },
      "outputs": [],
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, optimizer, checkpoint_path, best_model_path):\n",
        "  valid_loss_min=np.Inf\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss=0\n",
        "    valid_loss=0\n",
        "    model.train()\n",
        "\n",
        "    #training loop\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for index, batch in enumerate(training_loader):\n",
        "      print(' epoch', index)\n",
        "      input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "      attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "      targets = batch['targets'].to(device, dtype=torch.float)\n",
        "\n",
        "      # Model outputs\n",
        "      output = model(input_ids, attention_mask)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      loss = loss_fn(output, targets)\n",
        "\n",
        "      if index%5000==0:\n",
        "         print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print('Before loss data in training', loss.item(), train_loss)\n",
        "      train_loss = train_loss + ((1/(index+1))*(loss.item()-train_loss))\n",
        "      print('After loss data in training', loss.item(), train_loss)\n",
        "\n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    \n",
        "    #validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for index, batch in enumerate(validation_loader):\n",
        "        input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
        "        targets = batch['targets'].to(device, dtype=torch.float)\n",
        "        output = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(output, targets)\n",
        "        valid_loss = valid_loss + ((1/(index+1))*(loss.item()-valid_loss))\n",
        "    \n",
        "      checkpoint = {\n",
        "          'epoch': epoch+1,\n",
        "          'valid_loss_min': valid_loss,\n",
        "          'state_dict': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "\n",
        "      save_checkpoint(checkpoint, False, checkpoint_path, best_model_path)\n",
        "\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        save_checkpoint(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, valid_loss))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aW_FZomAo_OO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            " epoch 0\n",
            "Epoch: 1, Training Loss:  0.7565498352050781\n",
            "Before loss data in training 0.7565498352050781 0\n",
            "After loss data in training 0.7565498352050781 0.7565498352050781\n",
            " epoch 1\n",
            "Before loss data in training 0.7017446160316467 0.7565498352050781\n",
            "After loss data in training 0.7017446160316467 0.7291472256183624\n",
            " epoch 2\n",
            "Before loss data in training 0.6635946035385132 0.7291472256183624\n",
            "After loss data in training 0.6635946035385132 0.707296351591746\n",
            " epoch 3\n",
            "Before loss data in training 0.6585714817047119 0.707296351591746\n",
            "After loss data in training 0.6585714817047119 0.6951151341199875\n",
            " epoch 4\n",
            "Before loss data in training 0.6828012466430664 0.6951151341199875\n",
            "After loss data in training 0.6828012466430664 0.6926523566246032\n",
            " epoch 5\n",
            "Before loss data in training 0.5852746367454529 0.6926523566246032\n",
            "After loss data in training 0.5852746367454529 0.6747560699780781\n",
            " epoch 6\n",
            "Before loss data in training 0.5975963473320007 0.6747560699780781\n",
            "After loss data in training 0.5975963473320007 0.6637332524572099\n",
            " epoch 7\n",
            "Before loss data in training 0.5996399521827698 0.6637332524572099\n",
            "After loss data in training 0.5996399521827698 0.655721589922905\n",
            " epoch 8\n",
            "Before loss data in training 0.5799216628074646 0.655721589922905\n",
            "After loss data in training 0.5799216628074646 0.6472993757989671\n",
            " epoch 9\n",
            "Before loss data in training 0.541763424873352 0.6472993757989671\n",
            "After loss data in training 0.541763424873352 0.6367457807064056\n",
            " epoch 10\n",
            "Before loss data in training 0.6023472547531128 0.6367457807064056\n",
            "After loss data in training 0.6023472547531128 0.633618641983379\n",
            " epoch 11\n",
            "Before loss data in training 0.5403196215629578 0.633618641983379\n",
            "After loss data in training 0.5403196215629578 0.6258437236150106\n",
            " epoch 12\n",
            "Before loss data in training 0.5328350067138672 0.6258437236150106\n",
            "After loss data in training 0.5328350067138672 0.6186892069303073\n",
            " epoch 13\n",
            "Before loss data in training 0.4982788860797882 0.6186892069303073\n",
            "After loss data in training 0.4982788860797882 0.6100884697266987\n",
            " epoch 14\n",
            "Before loss data in training 0.5093393325805664 0.6100884697266987\n",
            "After loss data in training 0.5093393325805664 0.6033718605836232\n",
            " epoch 15\n",
            "Before loss data in training 0.5380905270576477 0.6033718605836232\n",
            "After loss data in training 0.5380905270576477 0.5992917772382498\n",
            " epoch 16\n",
            "Before loss data in training 0.49324288964271545 0.5992917772382498\n",
            "After loss data in training 0.49324288964271545 0.5930536073796889\n",
            " epoch 17\n",
            "Before loss data in training 0.4763977527618408 0.5930536073796889\n",
            "After loss data in training 0.4763977527618408 0.5865727265675863\n",
            " epoch 18\n",
            "Before loss data in training 0.5683077573776245 0.5865727265675863\n",
            "After loss data in training 0.5683077573776245 0.5856114123996936\n",
            " epoch 19\n",
            "Before loss data in training 0.4971512258052826 0.5856114123996936\n",
            "After loss data in training 0.4971512258052826 0.5811884030699731\n",
            " epoch 20\n",
            "Before loss data in training 0.5763825178146362 0.5811884030699731\n",
            "After loss data in training 0.5763825178146362 0.5809595513911475\n",
            " epoch 21\n",
            "Before loss data in training 0.4621402323246002 0.5809595513911475\n",
            "After loss data in training 0.4621402323246002 0.575558673251759\n",
            " epoch 22\n",
            "Before loss data in training 0.4977872371673584 0.575558673251759\n",
            "After loss data in training 0.4977872371673584 0.5721773064654807\n",
            " epoch 23\n",
            "Before loss data in training 0.4410177171230316 0.5721773064654807\n",
            "After loss data in training 0.4410177171230316 0.566712323576212\n",
            " epoch 24\n",
            "Before loss data in training 0.47825732827186584 0.566712323576212\n",
            "After loss data in training 0.47825732827186584 0.5631741237640382\n",
            " epoch 25\n",
            "Before loss data in training 0.4327038526535034 0.5631741237640382\n",
            "After loss data in training 0.4327038526535034 0.558156036413633\n",
            " epoch 26\n",
            "Before loss data in training 0.5081148743629456 0.558156036413633\n",
            "After loss data in training 0.5081148743629456 0.5563026600413853\n",
            " epoch 27\n",
            "Before loss data in training 0.4143454134464264 0.5563026600413853\n",
            "After loss data in training 0.4143454134464264 0.5512327583772796\n",
            " epoch 28\n",
            "Before loss data in training 0.41766777634620667 0.5512327583772796\n",
            "After loss data in training 0.41766777634620667 0.5466270693417253\n",
            " epoch 29\n",
            "Before loss data in training 0.42265963554382324 0.5466270693417253\n",
            "After loss data in training 0.42265963554382324 0.542494821548462\n",
            " epoch 30\n",
            "Before loss data in training 0.36415255069732666 0.542494821548462\n",
            "After loss data in training 0.36415255069732666 0.536741845069393\n",
            " epoch 31\n",
            "Before loss data in training 0.47962823510169983 0.536741845069393\n",
            "After loss data in training 0.47962823510169983 0.5349570447579026\n",
            " epoch 32\n",
            "Before loss data in training 0.5201390385627747 0.5349570447579026\n",
            "After loss data in training 0.5201390385627747 0.5345080142671411\n",
            " epoch 33\n",
            "Before loss data in training 0.5435991883277893 0.5345080142671411\n",
            "After loss data in training 0.5435991883277893 0.5347754017395131\n",
            " epoch 34\n",
            "Before loss data in training 0.4481469690799713 0.5347754017395131\n",
            "After loss data in training 0.4481469690799713 0.5323003036635262\n",
            " epoch 35\n",
            "Before loss data in training 0.4648594558238983 0.5323003036635262\n",
            "After loss data in training 0.4648594558238983 0.5304269467790921\n",
            " epoch 36\n",
            "Before loss data in training 0.4092661440372467 0.5304269467790921\n",
            "After loss data in training 0.4092661440372467 0.527152330488772\n",
            " epoch 37\n",
            "Before loss data in training 0.446260929107666 0.527152330488772\n",
            "After loss data in training 0.446260929107666 0.5250236093997955\n",
            " epoch 38\n",
            "Before loss data in training 0.38487526774406433 0.5250236093997955\n",
            "After loss data in training 0.38487526774406433 0.5214300621778537\n",
            " epoch 39\n",
            "Before loss data in training 0.4270402193069458 0.5214300621778537\n",
            "After loss data in training 0.4270402193069458 0.519070316106081\n",
            " epoch 40\n",
            "Before loss data in training 0.4482060670852661 0.519070316106081\n",
            "After loss data in training 0.4482060670852661 0.5173419197885002\n",
            " epoch 41\n",
            "Before loss data in training 0.38895997405052185 0.5173419197885002\n",
            "After loss data in training 0.38895997405052185 0.5142852067947388\n",
            " epoch 42\n",
            "Before loss data in training 0.3748752772808075 0.5142852067947388\n",
            "After loss data in training 0.3748752772808075 0.5110431154106938\n",
            " epoch 43\n",
            "Before loss data in training 0.46547165513038635 0.5110431154106938\n",
            "After loss data in training 0.46547165513038635 0.5100074004043231\n",
            " epoch 44\n",
            "Before loss data in training 0.38990965485572815 0.5100074004043231\n",
            "After loss data in training 0.38990965485572815 0.5073385616143543\n",
            " epoch 45\n",
            "Before loss data in training 0.42617499828338623 0.5073385616143543\n",
            "After loss data in training 0.42617499828338623 0.5055741363245506\n",
            " epoch 46\n",
            "Before loss data in training 0.3815796673297882 0.5055741363245506\n",
            "After loss data in training 0.3815796673297882 0.5029359561331727\n",
            " epoch 47\n",
            "Before loss data in training 0.3707965314388275 0.5029359561331727\n",
            "After loss data in training 0.3707965314388275 0.5001830514520405\n",
            " epoch 48\n",
            "Before loss data in training 0.38014283776283264 0.5001830514520405\n",
            "After loss data in training 0.38014283776283264 0.49773325117266887\n",
            " epoch 49\n",
            "Before loss data in training 0.4100671112537384 0.49773325117266887\n",
            "After loss data in training 0.4100671112537384 0.4959799283742903\n",
            " epoch 50\n",
            "Before loss data in training 0.3650093078613281 0.4959799283742903\n",
            "After loss data in training 0.3650093078613281 0.49341187699168315\n",
            " epoch 51\n",
            "Before loss data in training 0.38454344868659973 0.49341187699168315\n",
            "After loss data in training 0.38454344868659973 0.49131825337043156\n",
            " epoch 52\n",
            "Before loss data in training 0.3236078917980194 0.49131825337043156\n",
            "After loss data in training 0.3236078917980194 0.48815390692566907\n",
            " epoch 53\n",
            "Before loss data in training 0.4498499631881714 0.48815390692566907\n",
            "After loss data in training 0.4498499631881714 0.4874445746342339\n",
            " epoch 54\n",
            "Before loss data in training 0.35264861583709717 0.4874445746342339\n",
            "After loss data in training 0.35264861583709717 0.4849937390197405\n",
            " epoch 55\n",
            "Before loss data in training 0.37206849455833435 0.4849937390197405\n",
            "After loss data in training 0.37206849455833435 0.4829772167972154\n",
            " epoch 56\n",
            "Before loss data in training 0.3956495225429535 0.4829772167972154\n",
            "After loss data in training 0.3956495225429535 0.4814451519857371\n",
            " epoch 57\n",
            "Before loss data in training 0.4588545858860016 0.4814451519857371\n",
            "After loss data in training 0.4588545858860016 0.48105565946677614\n",
            " epoch 58\n",
            "Before loss data in training 0.47378063201904297 0.48105565946677614\n",
            "After loss data in training 0.47378063201904297 0.48093235391681455\n",
            " epoch 59\n",
            "Before loss data in training 0.3901652991771698 0.48093235391681455\n",
            "After loss data in training 0.3901652991771698 0.4794195696711538\n",
            " epoch 60\n",
            "Before loss data in training 0.35992738604545593 0.4794195696711538\n",
            "After loss data in training 0.35992738604545593 0.4774606814149948\n",
            " epoch 61\n",
            "Before loss data in training 0.3712826669216156 0.4774606814149948\n",
            "After loss data in training 0.3712826669216156 0.47574813279413386\n",
            " epoch 62\n",
            "Before loss data in training 0.412322074174881 0.47574813279413386\n",
            "After loss data in training 0.412322074174881 0.47474136995890764\n",
            " epoch 63\n",
            "Before loss data in training 0.3264515697956085 0.47474136995890764\n",
            "After loss data in training 0.3264515697956085 0.4724243418313561\n",
            " epoch 64\n",
            "Before loss data in training 0.35688507556915283 0.4724243418313561\n",
            "After loss data in training 0.35688507556915283 0.47064681465809144\n",
            " epoch 65\n",
            "Before loss data in training 0.34370800852775574 0.47064681465809144\n",
            "After loss data in training 0.34370800852775574 0.4687234994136924\n",
            " epoch 66\n",
            "Before loss data in training 0.3621128797531128 0.4687234994136924\n",
            "After loss data in training 0.3621128797531128 0.46713229613517626\n",
            " epoch 67\n",
            "Before loss data in training 0.36165380477905273 0.46713229613517626\n",
            "After loss data in training 0.36165380477905273 0.4655811418505274\n",
            " epoch 68\n",
            "Before loss data in training 0.41093429923057556 0.4655811418505274\n",
            "After loss data in training 0.41093429923057556 0.46478915862415127\n",
            " epoch 69\n",
            "Before loss data in training 0.346883088350296 0.46478915862415127\n",
            "After loss data in training 0.346883088350296 0.4631047861916676\n",
            " epoch 70\n",
            "Before loss data in training 0.32210299372673035 0.4631047861916676\n",
            "After loss data in training 0.32210299372673035 0.46111884545272486\n",
            " epoch 71\n",
            "Before loss data in training 0.5137935876846313 0.46111884545272486\n",
            "After loss data in training 0.5137935876846313 0.4618504390948347\n",
            " epoch 72\n",
            "Before loss data in training 0.2854576110839844 0.4618504390948347\n",
            "After loss data in training 0.2854576110839844 0.459434098985097\n",
            " epoch 73\n",
            "Before loss data in training 0.42538881301879883 0.459434098985097\n",
            "After loss data in training 0.42538881301879883 0.45897402755312\n",
            " epoch 74\n",
            "Before loss data in training 0.37568894028663635 0.45897402755312\n",
            "After loss data in training 0.37568894028663635 0.4578635597229002\n",
            " epoch 75\n",
            "Before loss data in training 0.3284086585044861 0.4578635597229002\n",
            "After loss data in training 0.3284086585044861 0.4561602057595\n",
            " epoch 76\n",
            "Before loss data in training 0.3691697418689728 0.4561602057595\n",
            "After loss data in training 0.3691697418689728 0.4550304594752074\n",
            " epoch 77\n",
            "Before loss data in training 0.4732576310634613 0.4550304594752074\n",
            "After loss data in training 0.4732576310634613 0.45526414116223635\n",
            " epoch 78\n",
            "Before loss data in training 0.4007573425769806 0.45526414116223635\n",
            "After loss data in training 0.4007573425769806 0.4545741816864736\n",
            " epoch 79\n",
            "Before loss data in training 0.4006706476211548 0.4545741816864736\n",
            "After loss data in training 0.4006706476211548 0.45390038751065714\n",
            " epoch 80\n",
            "Before loss data in training 0.4916134178638458 0.45390038751065714\n",
            "After loss data in training 0.4916134178638458 0.45436598047798044\n",
            " epoch 81\n",
            "Before loss data in training 0.49418219923973083 0.45436598047798044\n",
            "After loss data in training 0.49418219923973083 0.45485154412141643\n",
            " epoch 82\n",
            "Before loss data in training 0.3754791021347046 0.45485154412141643\n",
            "After loss data in training 0.3754791021347046 0.45389524963964883\n",
            " epoch 83\n",
            "Before loss data in training 0.33902132511138916 0.45389524963964883\n",
            "After loss data in training 0.33902132511138916 0.4525277029190743\n",
            " epoch 84\n",
            "Before loss data in training 0.3615989685058594 0.4525277029190743\n",
            "After loss data in training 0.3615989685058594 0.45145795310244824\n",
            " epoch 85\n",
            "Before loss data in training 0.3422534763813019 0.45145795310244824\n",
            "After loss data in training 0.3422534763813019 0.4501881336056907\n",
            " epoch 86\n",
            "Before loss data in training 0.28719279170036316 0.4501881336056907\n",
            "After loss data in training 0.28719279170036316 0.448314623928618\n",
            " epoch 87\n",
            "Before loss data in training 0.438713937997818 0.448314623928618\n",
            "After loss data in training 0.438713937997818 0.4482055252248589\n",
            " epoch 88\n",
            "Before loss data in training 0.3133697211742401 0.4482055252248589\n",
            "After loss data in training 0.3133697211742401 0.4466905161905823\n",
            " epoch 89\n",
            "Before loss data in training 0.3380252420902252 0.4466905161905823\n",
            "After loss data in training 0.3380252420902252 0.4454831242561339\n",
            " epoch 90\n",
            "Before loss data in training 0.2943357825279236 0.4454831242561339\n",
            "After loss data in training 0.2943357825279236 0.4438221644569228\n",
            " epoch 91\n",
            "Before loss data in training 0.3778034746646881 0.4438221644569228\n",
            "After loss data in training 0.3778034746646881 0.44310457000265935\n",
            " epoch 92\n",
            "Before loss data in training 0.31412312388420105 0.44310457000265935\n",
            "After loss data in training 0.31412312388420105 0.4417176727325684\n",
            " epoch 93\n",
            "Before loss data in training 0.26181742548942566 0.4417176727325684\n",
            "After loss data in training 0.26181742548942566 0.4398038403150882\n",
            " epoch 94\n",
            "Before loss data in training 0.37285545468330383 0.4398038403150882\n",
            "After loss data in training 0.37285545468330383 0.43909912046633254\n",
            " epoch 95\n",
            "Before loss data in training 0.3542332351207733 0.43909912046633254\n",
            "After loss data in training 0.3542332351207733 0.4382151008273163\n",
            " epoch 96\n",
            "Before loss data in training 0.4064117968082428 0.4382151008273163\n",
            "After loss data in training 0.4064117968082428 0.4378872317137176\n",
            " epoch 97\n",
            "Before loss data in training 0.4239908754825592 0.4378872317137176\n",
            "After loss data in training 0.4239908754825592 0.4377454321603384\n",
            " epoch 98\n",
            "Before loss data in training 0.22540901601314545 0.4377454321603384\n",
            "After loss data in training 0.22540901601314545 0.43560061987602333\n",
            " epoch 99\n",
            "Before loss data in training 0.36259856820106506 0.43560061987602333\n",
            "After loss data in training 0.36259856820106506 0.43487059935927375\n",
            " epoch 100\n",
            "Before loss data in training 0.48081517219543457 0.43487059935927375\n",
            "After loss data in training 0.48081517219543457 0.4353254961200278\n",
            " epoch 101\n",
            "Before loss data in training 0.49864354729652405 0.4353254961200278\n",
            "After loss data in training 0.49864354729652405 0.4359462613276405\n",
            " epoch 102\n",
            "Before loss data in training 0.3329838812351227 0.4359462613276405\n",
            "After loss data in training 0.3329838812351227 0.43494662656946076\n",
            " epoch 103\n",
            "Before loss data in training 0.547667920589447 0.43494662656946076\n",
            "After loss data in training 0.547667920589447 0.4360304851658068\n",
            " epoch 104\n",
            "Before loss data in training 0.43693891167640686 0.4360304851658068\n",
            "After loss data in training 0.43693891167640686 0.43603913684686013\n",
            " epoch 105\n",
            "Before loss data in training 0.2687400281429291 0.43603913684686013\n",
            "After loss data in training 0.2687400281429291 0.43446084336852114\n",
            " epoch 106\n",
            "Before loss data in training 0.4356168210506439 0.43446084336852114\n",
            "After loss data in training 0.4356168210506439 0.4344716468982606\n",
            " epoch 107\n",
            "Before loss data in training 0.31693345308303833 0.4344716468982606\n",
            "After loss data in training 0.31693345308303833 0.4333833302888604\n",
            " epoch 108\n",
            "Before loss data in training 0.3159047067165375 0.4333833302888604\n",
            "After loss data in training 0.3159047067165375 0.4323055447514996\n",
            " epoch 109\n",
            "Before loss data in training 0.4243224561214447 0.4323055447514996\n",
            "After loss data in training 0.4243224561214447 0.43223297121849913\n",
            " epoch 110\n",
            "Before loss data in training 0.3170628249645233 0.43223297121849913\n",
            "After loss data in training 0.3170628249645233 0.43119540233332815\n",
            " epoch 111\n",
            "Before loss data in training 0.33818888664245605 0.43119540233332815\n",
            "After loss data in training 0.33818888664245605 0.43036498701465964\n",
            " epoch 112\n",
            "Before loss data in training 0.2796463072299957 0.43036498701465964\n",
            "After loss data in training 0.2796463072299957 0.4290311933882467\n",
            " epoch 113\n",
            "Before loss data in training 0.3372800648212433 0.4290311933882467\n",
            "After loss data in training 0.3372800648212433 0.4282263589271326\n",
            " epoch 114\n",
            "Before loss data in training 0.3583754003047943 0.4282263589271326\n",
            "After loss data in training 0.3583754003047943 0.4276189592869384\n",
            " epoch 115\n",
            "Before loss data in training 0.3476327955722809 0.4276189592869384\n",
            "After loss data in training 0.3476327955722809 0.4269294233928465\n",
            " epoch 116\n",
            "Before loss data in training 0.2781757712364197 0.4269294233928465\n",
            "After loss data in training 0.2781757712364197 0.4256580246564668\n",
            " epoch 117\n",
            "Before loss data in training 0.4098222255706787 0.4256580246564668\n",
            "After loss data in training 0.4098222255706787 0.4255238229692991\n",
            " epoch 118\n",
            "Before loss data in training 0.2527402937412262 0.4255238229692991\n",
            "After loss data in training 0.2527402937412262 0.4240718605388111\n",
            " epoch 119\n",
            "Before loss data in training 0.38358521461486816 0.4240718605388111\n",
            "After loss data in training 0.38358521461486816 0.42373447182277824\n",
            " epoch 120\n",
            "Before loss data in training 0.42629918456077576 0.42373447182277824\n",
            "After loss data in training 0.42629918456077576 0.4237556677958195\n",
            " epoch 121\n",
            "Before loss data in training 0.3787592649459839 0.4237556677958195\n",
            "After loss data in training 0.3787592649459839 0.4233868448216405\n",
            " epoch 122\n",
            "Before loss data in training 0.2636233866214752 0.4233868448216405\n",
            "After loss data in training 0.2636233866214752 0.4220879549175741\n",
            " epoch 123\n",
            "Before loss data in training 0.3878486156463623 0.4220879549175741\n",
            "After loss data in training 0.3878486156463623 0.421811831213774\n",
            " epoch 124\n",
            "Before loss data in training 0.29108455777168274 0.421811831213774\n",
            "After loss data in training 0.29108455777168274 0.4207660130262373\n",
            " epoch 125\n",
            "Before loss data in training 0.2968128025531769 0.4207660130262373\n",
            "After loss data in training 0.2968128025531769 0.4197822573875622\n",
            " epoch 126\n",
            "Before loss data in training 0.31281501054763794 0.4197822573875622\n",
            "After loss data in training 0.31281501054763794 0.41893999560142103\n",
            " epoch 127\n",
            "Before loss data in training 0.32384148240089417 0.41893999560142103\n",
            "After loss data in training 0.32384148240089417 0.4181970384670419\n",
            " epoch 128\n",
            "Before loss data in training 0.30153343081474304 0.4181970384670419\n",
            "After loss data in training 0.30153343081474304 0.4172926694154737\n",
            " epoch 129\n",
            "Before loss data in training 0.47743770480155945 0.4172926694154737\n",
            "After loss data in training 0.47743770480155945 0.4177553235338282\n",
            " epoch 130\n",
            "Before loss data in training 0.3163183927536011 0.4177553235338282\n",
            "After loss data in training 0.3163183927536011 0.4169809958179486\n",
            " epoch 131\n",
            "Before loss data in training 0.26624542474746704 0.4169809958179486\n",
            "After loss data in training 0.26624542474746704 0.4158390596734753\n",
            " epoch 132\n",
            "Before loss data in training 0.4005241096019745 0.4158390596734753\n",
            "After loss data in training 0.4005241096019745 0.4157239096729377\n",
            " epoch 133\n",
            "Before loss data in training 0.370311975479126 0.4157239096729377\n",
            "After loss data in training 0.370311975479126 0.4153850146416406\n",
            " epoch 134\n",
            "Before loss data in training 0.36185312271118164 0.4153850146416406\n",
            "After loss data in training 0.36185312271118164 0.4149884821088224\n",
            " epoch 135\n",
            "Before loss data in training 0.4082145690917969 0.4149884821088224\n",
            "After loss data in training 0.4082145690917969 0.41493867392487366\n",
            " epoch 136\n",
            "Before loss data in training 0.3906761407852173 0.41493867392487366\n",
            "After loss data in training 0.3906761407852173 0.41476157514283235\n",
            " epoch 137\n",
            "Before loss data in training 0.4916253387928009 0.41476157514283235\n",
            "After loss data in training 0.4916253387928009 0.41531855893739733\n",
            " epoch 138\n",
            "Before loss data in training 0.3070015609264374 0.41531855893739733\n",
            "After loss data in training 0.3070015609264374 0.4145392999589012\n",
            " epoch 139\n",
            "Before loss data in training 0.37495359778404236 0.4145392999589012\n",
            "After loss data in training 0.37495359778404236 0.4142565449433665\n",
            " epoch 140\n",
            "Before loss data in training 0.24725711345672607 0.4142565449433665\n",
            "After loss data in training 0.24725711345672607 0.4130721518122556\n",
            " epoch 141\n",
            "Before loss data in training 0.2873609960079193 0.4130721518122556\n",
            "After loss data in training 0.2873609960079193 0.4121868619826476\n",
            " epoch 142\n",
            "Before loss data in training 0.4655592143535614 0.4121868619826476\n",
            "After loss data in training 0.4655592143535614 0.4125600952160106\n",
            " epoch 143\n",
            "Before loss data in training 0.2946053743362427 0.4125600952160106\n",
            "After loss data in training 0.2946053743362427 0.4117409652099011\n",
            " epoch 144\n",
            "Before loss data in training 0.36400899291038513 0.4117409652099011\n",
            "After loss data in training 0.36400899291038513 0.41141177919404237\n",
            " epoch 145\n",
            "Before loss data in training 0.45842352509498596 0.41141177919404237\n",
            "After loss data in training 0.45842352509498596 0.4117337774536379\n",
            " epoch 146\n",
            "Before loss data in training 0.2843460738658905 0.4117337774536379\n",
            "After loss data in training 0.2843460738658905 0.41086719443603414\n",
            " epoch 147\n",
            "Before loss data in training 0.22143645584583282 0.41086719443603414\n",
            "After loss data in training 0.22143645584583282 0.40958725701312737\n",
            " epoch 148\n",
            "Before loss data in training 0.23051069676876068 0.40958725701312737\n",
            "After loss data in training 0.23051069676876068 0.4083854009041048\n",
            " epoch 149\n",
            "Before loss data in training 0.4084005653858185 0.4083854009041048\n",
            "After loss data in training 0.4084005653858185 0.40838550200064955\n",
            " epoch 150\n",
            "Before loss data in training 0.5401238799095154 0.40838550200064955\n",
            "After loss data in training 0.5401238799095154 0.40925794158945\n",
            " epoch 151\n",
            "Before loss data in training 0.47215771675109863 0.40925794158945\n",
            "After loss data in training 0.47215771675109863 0.409671755899724\n",
            " epoch 152\n",
            "Before loss data in training 0.32921406626701355 0.409671755899724\n",
            "After loss data in training 0.32921406626701355 0.4091458886472226\n",
            " epoch 153\n",
            "Before loss data in training 0.22826051712036133 0.4091458886472226\n",
            "After loss data in training 0.22826051712036133 0.4079713083126326\n",
            " epoch 154\n",
            "Before loss data in training 0.3148755133152008 0.4079713083126326\n",
            "After loss data in training 0.3148755133152008 0.40737069028039113\n",
            " epoch 155\n",
            "Before loss data in training 0.29381054639816284 0.40737069028039113\n",
            "After loss data in training 0.29381054639816284 0.40664274064012046\n",
            " epoch 156\n",
            "Before loss data in training 0.4667070209980011 0.40664274064012046\n",
            "After loss data in training 0.4667070209980011 0.4070253156742471\n",
            " epoch 157\n",
            "Before loss data in training 0.27650436758995056 0.4070253156742471\n",
            "After loss data in training 0.27650436758995056 0.40619923372434646\n",
            " epoch 158\n",
            "Before loss data in training 0.42380180954933167 0.40619923372434646\n",
            "After loss data in training 0.42380180954933167 0.40630994174840296\n",
            " epoch 159\n",
            "Before loss data in training 0.32921302318573 0.40630994174840296\n",
            "After loss data in training 0.32921302318573 0.40582808600738624\n",
            " epoch 160\n",
            "Before loss data in training 0.379626601934433 0.40582808600738624\n",
            "After loss data in training 0.379626601934433 0.40566534387028713\n",
            " epoch 161\n",
            "Before loss data in training 0.31744465231895447 0.40566534387028713\n",
            "After loss data in training 0.31744465231895447 0.40512077170021715\n",
            " epoch 162\n",
            "Before loss data in training 0.3027944266796112 0.40512077170021715\n",
            "After loss data in training 0.3027944266796112 0.40449300271236066\n",
            " epoch 163\n",
            "Before loss data in training 0.3170357048511505 0.40449300271236066\n",
            "After loss data in training 0.3170357048511505 0.40395972650588985\n",
            " epoch 164\n",
            "Before loss data in training 0.3687257468700409 0.40395972650588985\n",
            "After loss data in training 0.3687257468700409 0.40374618723536954\n",
            " epoch 165\n",
            "Before loss data in training 0.3843434751033783 0.40374618723536954\n",
            "After loss data in training 0.3843434751033783 0.4036293034273455\n",
            " epoch 166\n",
            "Before loss data in training 0.39070478081703186 0.4036293034273455\n",
            "After loss data in training 0.39070478081703186 0.40355191107638555\n",
            " epoch 167\n",
            "Before loss data in training 0.2476639598608017 0.40355191107638555\n",
            "After loss data in training 0.2476639598608017 0.4026240066048642\n",
            " epoch 168\n",
            "Before loss data in training 0.34704044461250305 0.4026240066048642\n",
            "After loss data in training 0.34704044461250305 0.4022951097883413\n",
            " epoch 169\n",
            "Before loss data in training 0.36565545201301575 0.4022951097883413\n",
            "After loss data in training 0.36565545201301575 0.40207958238966296\n",
            " epoch 170\n",
            "Before loss data in training 0.3262055516242981 0.40207958238966296\n",
            "After loss data in training 0.3262055516242981 0.40163587460740935\n",
            " epoch 171\n",
            "Before loss data in training 0.24357259273529053 0.40163587460740935\n",
            "After loss data in training 0.24357259273529053 0.4007169020383854\n",
            " epoch 172\n",
            "Before loss data in training 0.23992480337619781 0.4007169020383854\n",
            "After loss data in training 0.23992480337619781 0.39978746794207215\n",
            " epoch 173\n",
            "Before loss data in training 0.29860565066337585 0.39978746794207215\n",
            "After loss data in training 0.29860565066337585 0.39920596324506813\n",
            " epoch 174\n",
            "Before loss data in training 0.5166142582893372 0.39920596324506813\n",
            "After loss data in training 0.5166142582893372 0.3998768677881782\n",
            " epoch 175\n",
            "Before loss data in training 0.33898308873176575 0.3998768677881782\n",
            "After loss data in training 0.33898308873176575 0.3995308804071759\n",
            " epoch 176\n",
            "Before loss data in training 0.36813104152679443 0.3995308804071759\n",
            "After loss data in training 0.36813104152679443 0.3993534801875127\n",
            " epoch 177\n",
            "Before loss data in training 0.35749951004981995 0.3993534801875127\n",
            "After loss data in training 0.35749951004981995 0.3991183455238178\n",
            " epoch 178\n",
            "Before loss data in training 0.34346333146095276 0.3991183455238178\n",
            "After loss data in training 0.34346333146095276 0.39880742365754485\n",
            " epoch 179\n",
            "Before loss data in training 0.3907507658004761 0.39880742365754485\n",
            "After loss data in training 0.3907507658004761 0.3987626644472278\n",
            " epoch 180\n",
            "Before loss data in training 0.31453731656074524 0.3987626644472278\n",
            "After loss data in training 0.31453731656074524 0.39829733103349035\n",
            " epoch 181\n",
            "Before loss data in training 0.32548609375953674 0.39829733103349035\n",
            "After loss data in training 0.32548609375953674 0.3978972692902269\n",
            " epoch 182\n",
            "Before loss data in training 0.31056758761405945 0.3978972692902269\n",
            "After loss data in training 0.31056758761405945 0.3974200579149473\n",
            " epoch 183\n",
            "Before loss data in training 0.31382885575294495 0.3974200579149473\n",
            "After loss data in training 0.31382885575294495 0.39696575790319727\n",
            " epoch 184\n",
            "Before loss data in training 0.26428675651550293 0.39696575790319727\n",
            "After loss data in training 0.26428675651550293 0.39624857411191244\n",
            " epoch 185\n",
            "Before loss data in training 0.39458373188972473 0.39624857411191244\n",
            "After loss data in training 0.39458373188972473 0.396239623347277\n",
            " epoch 186\n",
            "Before loss data in training 0.2987392246723175 0.396239623347277\n",
            "After loss data in training 0.2987392246723175 0.39571823084099383\n",
            " epoch 187\n",
            "Before loss data in training 0.34317395091056824 0.39571823084099383\n",
            "After loss data in training 0.34317395091056824 0.3954387399903001\n",
            " epoch 188\n",
            "Before loss data in training 0.33339598774909973 0.3954387399903001\n",
            "After loss data in training 0.33339598774909973 0.3951104714599234\n",
            " epoch 189\n",
            "Before loss data in training 0.4308265745639801 0.3951104714599234\n",
            "After loss data in training 0.4308265745639801 0.39529845094994476\n",
            " epoch 190\n",
            "Before loss data in training 0.3848108947277069 0.39529845094994476\n",
            "After loss data in training 0.3848108947277069 0.3952435422786241\n",
            " epoch 191\n",
            "Before loss data in training 0.38907983899116516 0.3952435422786241\n",
            "After loss data in training 0.38907983899116516 0.3952114396573353\n",
            " epoch 192\n",
            "Before loss data in training 0.4863458573818207 0.3952114396573353\n",
            "After loss data in training 0.4863458573818207 0.3956836387129026\n",
            " epoch 193\n",
            "Before loss data in training 0.44305041432380676 0.3956836387129026\n",
            "After loss data in training 0.44305041432380676 0.39592779735007216\n",
            " epoch 194\n",
            "Before loss data in training 0.5576584935188293 0.39592779735007216\n",
            "After loss data in training 0.5576584935188293 0.39675718553555295\n",
            " epoch 195\n",
            "Before loss data in training 0.3431505262851715 0.39675718553555295\n",
            "After loss data in training 0.3431505262851715 0.3964836821720306\n",
            " epoch 196\n",
            "Before loss data in training 0.36641931533813477 0.3964836821720306\n",
            "After loss data in training 0.36641931533813477 0.3963310711728738\n",
            " epoch 197\n",
            "Before loss data in training 0.30221766233444214 0.3963310711728738\n",
            "After loss data in training 0.30221766233444214 0.39585575092621506\n",
            " epoch 198\n",
            "Before loss data in training 0.3138827681541443 0.39585575092621506\n",
            "After loss data in training 0.3138827681541443 0.39544382638967196\n",
            " epoch 199\n",
            "Before loss data in training 0.2671760320663452 0.39544382638967196\n",
            "After loss data in training 0.2671760320663452 0.39480248741805535\n",
            " epoch 200\n",
            "Before loss data in training 0.4084189832210541 0.39480248741805535\n",
            "After loss data in training 0.4084189832210541 0.39487023117826925\n",
            " epoch 201\n",
            "Before loss data in training 0.395569771528244 0.39487023117826925\n",
            "After loss data in training 0.395569771528244 0.39487369424930874\n",
            " epoch 202\n",
            "Before loss data in training 0.3045257031917572 0.39487369424930874\n",
            "After loss data in training 0.3045257031917572 0.39442863025395136\n",
            " epoch 203\n",
            "Before loss data in training 0.34868863224983215 0.39442863025395136\n",
            "After loss data in training 0.34868863224983215 0.3942044145774606\n",
            " epoch 204\n",
            "Before loss data in training 0.5198688507080078 0.3942044145774606\n",
            "After loss data in training 0.5198688507080078 0.3948174118268779\n",
            " epoch 205\n",
            "Before loss data in training 0.36831533908843994 0.3948174118268779\n",
            "After loss data in training 0.36831533908843994 0.3946887609883418\n",
            " epoch 206\n",
            "Before loss data in training 0.32167938351631165 0.3946887609883418\n",
            "After loss data in training 0.32167938351631165 0.3943360586817137\n",
            " epoch 207\n",
            "Before loss data in training 0.47783398628234863 0.3943360586817137\n",
            "After loss data in training 0.47783398628234863 0.3947374910259475\n",
            " epoch 208\n",
            "Before loss data in training 0.3519645631313324 0.3947374910259475\n",
            "After loss data in training 0.3519645631313324 0.39453283586855703\n",
            " epoch 209\n",
            "Before loss data in training 0.2837560772895813 0.39453283586855703\n",
            "After loss data in training 0.2837560772895813 0.39400532749437145\n",
            " epoch 210\n",
            "Before loss data in training 0.5505928993225098 0.39400532749437145\n",
            "After loss data in training 0.5505928993225098 0.39474744868786976\n",
            " epoch 211\n",
            "Before loss data in training 0.3425241708755493 0.39474744868786976\n",
            "After loss data in training 0.3425241708755493 0.3945011124717739\n",
            " epoch 212\n",
            "Before loss data in training 0.5138862729072571 0.3945011124717739\n",
            "After loss data in training 0.5138862729072571 0.3950616061827386\n",
            " epoch 213\n",
            "Before loss data in training 0.4340309798717499 0.3950616061827386\n",
            "After loss data in training 0.4340309798717499 0.3952437060597901\n",
            " epoch 214\n",
            "Before loss data in training 0.34334418177604675 0.3952437060597901\n",
            "After loss data in training 0.34334418177604675 0.3950023129235866\n",
            " epoch 215\n",
            "Before loss data in training 0.4185698926448822 0.3950023129235866\n",
            "After loss data in training 0.4185698926448822 0.395111422088963\n",
            " epoch 216\n",
            "Before loss data in training 0.36860594153404236 0.395111422088963\n",
            "After loss data in training 0.36860594153404236 0.3949892770172813\n",
            " epoch 217\n",
            "Before loss data in training 0.4259696304798126 0.3949892770172813\n",
            "After loss data in training 0.4259696304798126 0.3951313887304122\n",
            " epoch 218\n",
            "Before loss data in training 0.25562921166419983 0.3951313887304122\n",
            "After loss data in training 0.25562921166419983 0.39449439248810075\n",
            " epoch 219\n",
            "Before loss data in training 0.48491355776786804 0.39449439248810075\n",
            "After loss data in training 0.48491355776786804 0.3949053886939179\n",
            " epoch 220\n",
            "Before loss data in training 0.3407052457332611 0.3949053886939179\n",
            "After loss data in training 0.3407052457332611 0.3946601391782588\n",
            " epoch 221\n",
            "Before loss data in training 0.32835623621940613 0.3946601391782588\n",
            "After loss data in training 0.32835623621940613 0.39436147294871443\n",
            " epoch 222\n",
            "Before loss data in training 0.39618948101997375 0.39436147294871443\n",
            "After loss data in training 0.39618948101997375 0.39436967029432546\n",
            " epoch 223\n",
            "Before loss data in training 0.3173280656337738 0.39436967029432546\n",
            "After loss data in training 0.3173280656337738 0.39402573455923373\n",
            " epoch 224\n",
            "Before loss data in training 0.33140113949775696 0.39402573455923373\n",
            "After loss data in training 0.33140113949775696 0.39374740302562716\n",
            " epoch 225\n",
            "Before loss data in training 0.4164002239704132 0.39374740302562716\n",
            "After loss data in training 0.4164002239704132 0.39384763674662177\n",
            " epoch 226\n",
            "Before loss data in training 0.3820548355579376 0.39384763674662177\n",
            "After loss data in training 0.3820548355579376 0.39379568608059234\n",
            " epoch 227\n",
            "Before loss data in training 0.2985006868839264 0.39379568608059234\n",
            "After loss data in training 0.2985006868839264 0.39337772555779993\n",
            " epoch 228\n",
            "Before loss data in training 0.43959879875183105 0.39337772555779993\n",
            "After loss data in training 0.43959879875183105 0.3935795643053721\n",
            " epoch 229\n",
            "Before loss data in training 0.3071722984313965 0.3935795643053721\n",
            "After loss data in training 0.3071722984313965 0.39320388054070265\n",
            " epoch 230\n",
            "Before loss data in training 0.2835734188556671 0.39320388054070265\n",
            "After loss data in training 0.2835734188556671 0.3927292897974774\n",
            " epoch 231\n",
            "Before loss data in training 0.2713969647884369 0.3927292897974774\n",
            "After loss data in training 0.2713969647884369 0.39220630563795567\n",
            " epoch 232\n",
            "Before loss data in training 0.2566828727722168 0.39220630563795567\n",
            "After loss data in training 0.2566828727722168 0.3916246600033388\n",
            " epoch 233\n",
            "Before loss data in training 0.342582106590271 0.3916246600033388\n",
            "After loss data in training 0.342582106590271 0.3914150764417445\n",
            " epoch 234\n",
            "Before loss data in training 0.3342755138874054 0.3914150764417445\n",
            "After loss data in training 0.3342755138874054 0.3911719293670452\n",
            " epoch 235\n",
            "Before loss data in training 0.37521275877952576 0.3911719293670452\n",
            "After loss data in training 0.37521275877952576 0.3911043057628608\n",
            " epoch 236\n",
            "Before loss data in training 0.39934757351875305 0.3911043057628608\n",
            "After loss data in training 0.39934757351875305 0.3911390874833498\n",
            " epoch 237\n",
            "Before loss data in training 0.4047963619232178 0.3911390874833498\n",
            "After loss data in training 0.4047963619232178 0.3911964709893997\n",
            " epoch 238\n",
            "Before loss data in training 0.31945836544036865 0.3911964709893997\n",
            "After loss data in training 0.31945836544036865 0.39089631155195603\n",
            " epoch 239\n",
            "Before loss data in training 0.3206835091114044 0.39089631155195603\n",
            "After loss data in training 0.3206835091114044 0.3906037582084537\n",
            " epoch 240\n",
            "Before loss data in training 0.4317048490047455 0.3906037582084537\n",
            "After loss data in training 0.4317048490047455 0.39077430215366654\n",
            " epoch 241\n",
            "Before loss data in training 0.30836841464042664 0.39077430215366654\n",
            "After loss data in training 0.30836841464042664 0.39043378195733086\n",
            " epoch 242\n",
            "Before loss data in training 0.4490538537502289 0.39043378195733086\n",
            "After loss data in training 0.4490538537502289 0.3906750168206761\n",
            " epoch 243\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/jt/gtp4f1sn19g4t4r5r42dd34m0000gn/T/ipykernel_33580/60328045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/curr_chckpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/best.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/var/folders/jt/gtp4f1sn19g4t4r5r42dd34m0000gn/T/ipykernel_33580/2740270290.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs, training_loader, validation_loader, model, optimizer, checkpoint_path, best_model_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Model outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/jt/gtp4f1sn19g4t4r5r42dd34m0000gn/T/ipykernel_33580/1512173231.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutput_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         )\n\u001b[0;32m--> 995\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    996\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[0;32m--> 401\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/envs/nlpmod/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, '/curr_chckpt', '/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtzDxuPLo_K2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpr-XpWPo9-I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MffsfxJo965"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoCceRr1o949"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JevAMjO3o91q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TtbMh4Po9zR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkKAaaxlo9ll"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CommentToxicityClassifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
