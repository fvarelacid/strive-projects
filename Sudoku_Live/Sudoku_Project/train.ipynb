{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0030\n",
      "\tIteration: 40\t Loss: 0.2448\n",
      "\tIteration: 80\t Loss: 0.2749\n",
      "\tIteration: 120\t Loss: 0.2983\n",
      "\tIteration: 160\t Loss: 0.2982\n",
      "\tIteration: 200\t Loss: 0.2788\n",
      "\tIteration: 240\t Loss: 0.2431\n",
      "\tIteration: 280\t Loss: 0.2711\n",
      "\tIteration: 320\t Loss: 0.2343\n",
      "\tIteration: 360\t Loss: 0.2435\n",
      "\tIteration: 400\t Loss: 0.3002\n",
      "\tIteration: 440\t Loss: 0.2851\n",
      "\tIteration: 480\t Loss: 0.2929\n",
      "\tIteration: 520\t Loss: 0.3076\n",
      "\tIteration: 560\t Loss: 0.3220\n",
      "\tIteration: 600\t Loss: 0.2872\n",
      "\tIteration: 640\t Loss: 0.2588\n",
      "\tIteration: 680\t Loss: 0.3009\n",
      "\tIteration: 720\t Loss: 0.3251\n",
      "\tIteration: 760\t Loss: 0.2782\n",
      "\tIteration: 800\t Loss: 0.2818\n",
      "\tIteration: 840\t Loss: 0.2578\n",
      "\tIteration: 880\t Loss: 0.2748\n",
      "\tIteration: 920\t Loss: 0.2954\n",
      "\tIteration: 960\t Loss: 0.2798\n",
      "\tIteration: 1000\t Loss: 0.3321\n",
      "\tIteration: 1040\t Loss: 0.2476\n",
      "\tIteration: 1080\t Loss: 0.3181\n",
      "\tIteration: 1120\t Loss: 0.2928\n",
      "\tIteration: 1160\t Loss: 0.2534\n",
      "\tIteration: 1200\t Loss: 0.3473\n",
      "\tIteration: 1240\t Loss: 0.3523\n",
      "\tIteration: 1280\t Loss: 0.2759\n",
      "\tIteration: 1320\t Loss: 0.3234\n",
      "\tIteration: 1360\t Loss: 0.2856\n",
      "\tIteration: 1400\t Loss: 0.2798\n",
      "\tIteration: 1440\t Loss: 0.2666\n",
      "\tIteration: 1480\t Loss: 0.2696\n",
      "\tIteration: 1520\t Loss: 0.2173\n",
      "\tIteration: 1560\t Loss: 0.2544\n",
      "\tIteration: 1600\t Loss: 0.2652\n",
      "\tIteration: 1640\t Loss: 0.2949\n",
      "\tIteration: 1680\t Loss: 0.2367\n",
      "\tIteration: 1720\t Loss: 0.2458\n",
      "\tIteration: 1760\t Loss: 0.3073\n",
      "\tIteration: 1800\t Loss: 0.2348\n",
      "\tIteration: 1840\t Loss: 0.2519\n",
      "\tIteration: 1880\t Loss: 0.2743\n",
      "\tIteration: 1920\t Loss: 0.2662\n",
      "\tIteration: 1960\t Loss: 0.3180\n",
      "\tIteration: 2000\t Loss: 0.2680\n",
      "\tIteration: 2040\t Loss: 0.2418\n",
      "\tIteration: 2080\t Loss: 0.3041\n",
      "\tIteration: 2120\t Loss: 0.2347\n",
      "\tIteration: 2160\t Loss: 0.3008\n",
      "\tIteration: 2200\t Loss: 0.2389\n",
      "\tIteration: 2240\t Loss: 0.2379\n",
      "\tIteration: 2280\t Loss: 0.2699\n",
      "\tIteration: 2320\t Loss: 0.2666\n",
      "\tIteration: 2360\t Loss: 0.2848\n",
      "\tIteration: 2400\t Loss: 0.2518\n",
      "\tIteration: 2440\t Loss: 0.2641\n",
      "\tIteration: 2480\t Loss: 0.3040\n",
      "\tIteration: 2520\t Loss: 0.2494\n",
      "\tIteration: 2560\t Loss: 0.2346\n",
      "\tIteration: 2600\t Loss: 0.2690\n",
      "\tIteration: 2640\t Loss: 0.2744\n",
      "\tIteration: 2680\t Loss: 0.2754\n",
      "\tIteration: 2720\t Loss: 0.2716\n",
      "\tIteration: 2760\t Loss: 0.3427\n",
      "\tIteration: 2800\t Loss: 0.2498\n",
      "\tIteration: 2840\t Loss: 0.2389\n",
      "\tIteration: 2880\t Loss: 0.2604\n",
      "\tIteration: 2920\t Loss: 0.2530\n",
      "\tIteration: 2960\t Loss: 0.2657\n",
      "\tIteration: 3000\t Loss: 0.3192\n",
      "\tIteration: 3040\t Loss: 0.2701\n",
      "\tIteration: 3080\t Loss: 0.2343\n",
      "\tIteration: 3120\t Loss: 0.2577\n",
      "\tIteration: 3160\t Loss: 0.3788\n",
      "\tIteration: 3200\t Loss: 0.2723\n",
      "\tIteration: 3240\t Loss: 0.2885\n",
      "\tIteration: 3280\t Loss: 0.2640\n",
      "\tIteration: 3320\t Loss: 0.2622\n",
      "\tIteration: 3360\t Loss: 0.2758\n",
      "\tIteration: 3400\t Loss: 0.3155\n",
      "\tIteration: 3440\t Loss: 0.2748\n",
      "\tIteration: 3480\t Loss: 0.2443\n",
      "\tIteration: 3520\t Loss: 0.2493\n",
      "\tIteration: 3560\t Loss: 0.2971\n",
      "\tIteration: 3600\t Loss: 0.2894\n",
      "\tIteration: 3640\t Loss: 0.2516\n",
      "\tIteration: 3680\t Loss: 0.3033\n",
      "\tIteration: 3720\t Loss: 0.2320\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0045\n",
      "\tIteration: 40\t Loss: 0.2370\n",
      "\tIteration: 80\t Loss: 0.2693\n",
      "\tIteration: 120\t Loss: 0.2774\n",
      "\tIteration: 160\t Loss: 0.2829\n",
      "\tIteration: 200\t Loss: 0.2373\n",
      "\tIteration: 240\t Loss: 0.2944\n",
      "\tIteration: 280\t Loss: 0.2567\n",
      "\tIteration: 320\t Loss: 0.2407\n",
      "\tIteration: 360\t Loss: 0.3054\n",
      "\tIteration: 400\t Loss: 0.2749\n",
      "\tIteration: 440\t Loss: 0.2774\n",
      "\tIteration: 480\t Loss: 0.2098\n",
      "\tIteration: 520\t Loss: 0.2311\n",
      "\tIteration: 560\t Loss: 0.3164\n",
      "\tIteration: 600\t Loss: 0.2138\n",
      "\tIteration: 640\t Loss: 0.2304\n",
      "\tIteration: 680\t Loss: 0.2353\n",
      "\tIteration: 720\t Loss: 0.2584\n",
      "\tIteration: 760\t Loss: 0.2288\n",
      "\tIteration: 800\t Loss: 0.2975\n",
      "\tIteration: 840\t Loss: 0.2718\n",
      "\tIteration: 880\t Loss: 0.3105\n",
      "\tIteration: 920\t Loss: 0.2620\n",
      "\tIteration: 960\t Loss: 0.2020\n",
      "\tIteration: 1000\t Loss: 0.2554\n",
      "\tIteration: 1040\t Loss: 0.2294\n",
      "\tIteration: 1080\t Loss: 0.2732\n",
      "\tIteration: 1120\t Loss: 0.2331\n",
      "\tIteration: 1160\t Loss: 0.3115\n",
      "\tIteration: 1200\t Loss: 0.2447\n",
      "\tIteration: 1240\t Loss: 0.2582\n",
      "\tIteration: 1280\t Loss: 0.2631\n",
      "\tIteration: 1320\t Loss: 0.2756\n",
      "\tIteration: 1360\t Loss: 0.2146\n",
      "\tIteration: 1400\t Loss: 0.2660\n",
      "\tIteration: 1440\t Loss: 0.2670\n",
      "\tIteration: 1480\t Loss: 0.2068\n",
      "\tIteration: 1520\t Loss: 0.1902\n",
      "\tIteration: 1560\t Loss: 0.2445\n",
      "\tIteration: 1600\t Loss: 0.2500\n",
      "\tIteration: 1640\t Loss: 0.2409\n",
      "\tIteration: 1680\t Loss: 0.2600\n",
      "\tIteration: 1720\t Loss: 0.1990\n",
      "\tIteration: 1760\t Loss: 0.2870\n",
      "\tIteration: 1800\t Loss: 0.2435\n",
      "\tIteration: 1840\t Loss: 0.2059\n",
      "\tIteration: 1880\t Loss: 0.2318\n",
      "\tIteration: 1920\t Loss: 0.2964\n",
      "\tIteration: 1960\t Loss: 0.2205\n",
      "\tIteration: 2000\t Loss: 0.2686\n",
      "\tIteration: 2040\t Loss: 0.2742\n",
      "\tIteration: 2080\t Loss: 0.2176\n",
      "\tIteration: 2120\t Loss: 0.2738\n",
      "\tIteration: 2160\t Loss: 0.2712\n",
      "\tIteration: 2200\t Loss: 0.2176\n",
      "\tIteration: 2240\t Loss: 0.2289\n",
      "\tIteration: 2280\t Loss: 0.2339\n",
      "\tIteration: 2320\t Loss: 0.2317\n",
      "\tIteration: 2360\t Loss: 0.2428\n",
      "\tIteration: 2400\t Loss: 0.2403\n",
      "\tIteration: 2440\t Loss: 0.2364\n",
      "\tIteration: 2480\t Loss: 0.2431\n",
      "\tIteration: 2520\t Loss: 0.2866\n",
      "\tIteration: 2560\t Loss: 0.2054\n",
      "\tIteration: 2600\t Loss: 0.2207\n",
      "\tIteration: 2640\t Loss: 0.2177\n",
      "\tIteration: 2680\t Loss: 0.2232\n",
      "\tIteration: 2720\t Loss: 0.2281\n",
      "\tIteration: 2760\t Loss: 0.2300\n",
      "\tIteration: 2800\t Loss: 0.2778\n",
      "\tIteration: 2840\t Loss: 0.2375\n",
      "\tIteration: 2880\t Loss: 0.2217\n",
      "\tIteration: 2920\t Loss: 0.1963\n",
      "\tIteration: 2960\t Loss: 0.2870\n",
      "\tIteration: 3000\t Loss: 0.2255\n",
      "\tIteration: 3040\t Loss: 0.2408\n",
      "\tIteration: 3080\t Loss: 0.2586\n",
      "\tIteration: 3120\t Loss: 0.2396\n",
      "\tIteration: 3160\t Loss: 0.3057\n",
      "\tIteration: 3200\t Loss: 0.2838\n",
      "\tIteration: 3240\t Loss: 0.2466\n",
      "\tIteration: 3280\t Loss: 0.2438\n",
      "\tIteration: 3320\t Loss: 0.2394\n",
      "\tIteration: 3360\t Loss: 0.2393\n",
      "\tIteration: 3400\t Loss: 0.2704\n",
      "\tIteration: 3440\t Loss: 0.2583\n",
      "\tIteration: 3480\t Loss: 0.2601\n",
      "\tIteration: 3520\t Loss: 0.2727\n",
      "\tIteration: 3560\t Loss: 0.2437\n",
      "\tIteration: 3600\t Loss: 0.2384\n",
      "\tIteration: 3640\t Loss: 0.2556\n",
      "\tIteration: 3680\t Loss: 0.2377\n",
      "\tIteration: 3720\t Loss: 0.2635\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0029\n",
      "\tIteration: 40\t Loss: 0.2030\n",
      "\tIteration: 80\t Loss: 0.2215\n",
      "\tIteration: 120\t Loss: 0.2137\n",
      "\tIteration: 160\t Loss: 0.2356\n",
      "\tIteration: 200\t Loss: 0.2036\n",
      "\tIteration: 240\t Loss: 0.2226\n",
      "\tIteration: 280\t Loss: 0.2285\n",
      "\tIteration: 320\t Loss: 0.2842\n",
      "\tIteration: 360\t Loss: 0.2198\n",
      "\tIteration: 400\t Loss: 0.2002\n",
      "\tIteration: 440\t Loss: 0.2137\n",
      "\tIteration: 480\t Loss: 0.2551\n",
      "\tIteration: 520\t Loss: 0.2106\n",
      "\tIteration: 560\t Loss: 0.2599\n",
      "\tIteration: 600\t Loss: 0.2511\n",
      "\tIteration: 640\t Loss: 0.2406\n",
      "\tIteration: 680\t Loss: 0.2257\n",
      "\tIteration: 720\t Loss: 0.2035\n",
      "\tIteration: 760\t Loss: 0.1959\n",
      "\tIteration: 800\t Loss: 0.2854\n",
      "\tIteration: 840\t Loss: 0.2307\n",
      "\tIteration: 880\t Loss: 0.2325\n",
      "\tIteration: 920\t Loss: 0.2229\n",
      "\tIteration: 960\t Loss: 0.2041\n",
      "\tIteration: 1000\t Loss: 0.2628\n",
      "\tIteration: 1040\t Loss: 0.2802\n",
      "\tIteration: 1080\t Loss: 0.2184\n",
      "\tIteration: 1120\t Loss: 0.1931\n",
      "\tIteration: 1160\t Loss: 0.3056\n",
      "\tIteration: 1200\t Loss: 0.2801\n",
      "\tIteration: 1240\t Loss: 0.2594\n",
      "\tIteration: 1280\t Loss: 0.2169\n",
      "\tIteration: 1320\t Loss: 0.2079\n",
      "\tIteration: 1360\t Loss: 0.1971\n",
      "\tIteration: 1400\t Loss: 0.2215\n",
      "\tIteration: 1440\t Loss: 0.2502\n",
      "\tIteration: 1480\t Loss: 0.2331\n",
      "\tIteration: 1520\t Loss: 0.1954\n",
      "\tIteration: 1560\t Loss: 0.2527\n",
      "\tIteration: 1600\t Loss: 0.2538\n",
      "\tIteration: 1640\t Loss: 0.1893\n",
      "\tIteration: 1680\t Loss: 0.2526\n",
      "\tIteration: 1720\t Loss: 0.2726\n",
      "\tIteration: 1760\t Loss: 0.2374\n",
      "\tIteration: 1800\t Loss: 0.2215\n",
      "\tIteration: 1840\t Loss: 0.2223\n",
      "\tIteration: 1880\t Loss: 0.2060\n",
      "\tIteration: 1920\t Loss: 0.2158\n",
      "\tIteration: 1960\t Loss: 0.2181\n",
      "\tIteration: 2000\t Loss: 0.2074\n",
      "\tIteration: 2040\t Loss: 0.2768\n",
      "\tIteration: 2080\t Loss: 0.2358\n",
      "\tIteration: 2120\t Loss: 0.1814\n",
      "\tIteration: 2160\t Loss: 0.2402\n",
      "\tIteration: 2200\t Loss: 0.2147\n",
      "\tIteration: 2240\t Loss: 0.2291\n",
      "\tIteration: 2280\t Loss: 0.1541\n",
      "\tIteration: 2320\t Loss: 0.2142\n",
      "\tIteration: 2360\t Loss: 0.2435\n",
      "\tIteration: 2400\t Loss: 0.2088\n",
      "\tIteration: 2440\t Loss: 0.2040\n",
      "\tIteration: 2480\t Loss: 0.2456\n",
      "\tIteration: 2520\t Loss: 0.1813\n",
      "\tIteration: 2560\t Loss: 0.1956\n",
      "\tIteration: 2600\t Loss: 0.2563\n",
      "\tIteration: 2640\t Loss: 0.2257\n",
      "\tIteration: 2680\t Loss: 0.2570\n",
      "\tIteration: 2720\t Loss: 0.2395\n",
      "\tIteration: 2760\t Loss: 0.2263\n",
      "\tIteration: 2800\t Loss: 0.2189\n",
      "\tIteration: 2840\t Loss: 0.2065\n",
      "\tIteration: 2880\t Loss: 0.2250\n",
      "\tIteration: 2920\t Loss: 0.2530\n",
      "\tIteration: 2960\t Loss: 0.2486\n",
      "\tIteration: 3000\t Loss: 0.2039\n",
      "\tIteration: 3040\t Loss: 0.2306\n",
      "\tIteration: 3080\t Loss: 0.2632\n",
      "\tIteration: 3120\t Loss: 0.2226\n",
      "\tIteration: 3160\t Loss: 0.2078\n",
      "\tIteration: 3200\t Loss: 0.2073\n",
      "\tIteration: 3240\t Loss: 0.2128\n",
      "\tIteration: 3280\t Loss: 0.1725\n",
      "\tIteration: 3320\t Loss: 0.2227\n",
      "\tIteration: 3360\t Loss: 0.2017\n",
      "\tIteration: 3400\t Loss: 0.1949\n",
      "\tIteration: 3440\t Loss: 0.2158\n",
      "\tIteration: 3480\t Loss: 0.2133\n",
      "\tIteration: 3520\t Loss: 0.2027\n",
      "\tIteration: 3560\t Loss: 0.2269\n",
      "\tIteration: 3600\t Loss: 0.2650\n",
      "\tIteration: 3640\t Loss: 0.2240\n",
      "\tIteration: 3680\t Loss: 0.1947\n",
      "\tIteration: 3720\t Loss: 0.2117\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "\n",
    "torch.save(model.state_dict(), \"output/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "\n",
    "view_classify(img.view(1, 28, 28), ps)\n",
    "\n",
    "print(ps.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7282691e-02, 2.1313112e-06, 2.3343969e-02, 3.3521052e-02,\n",
       "       7.1679242e-05, 8.6236540e-03, 6.8794098e-04, 6.5312047e-06,\n",
       "       9.0206951e-01, 4.3908213e-03], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ps.data.numpy().squeeze()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e16fa511924edcfa11d032a05c2d0ea5f7dff959ac46ae79302490986f7a6872"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('dlmod': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
